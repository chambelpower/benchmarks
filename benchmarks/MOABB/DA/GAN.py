import numpy as np
import os
import torch
import tensorflow as tf
import torch.nn as nn

os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # Silences the warning and error logs generated by TensorFlow

import matplotlib.pyplot as plt
from keras.optimizers import Adam
from keras.models import Sequential, Model
from keras.layers import Input, Dense, Flatten, LeakyReLU, Reshape, BatchNormalization

class GAN(nn.Module):
    def __init__(self, epochs=10, batchsize=2, sample_interval=10, channels=17, noise=100, dropout=0.2, alpha=0.2, momentum=0.8, adam_learning_rate=0.0002, adam_beta_1=0.2):
        super(GAN, self).__init__()
        # Dataset features
        self.epochs = epochs
        self.batchsize = batchsize
        self.sample_interval = sample_interval
        self.channels = channels
        self.time_stamp = 500  # Adjusted for the new time dimension of 500
        self.eeg_shape = (self.time_stamp, self.channels)

        # Model specific parameters (Noise generation, Dropout for overfitting reduction, etc...):
        self.noise = noise
        self.dropout = dropout
        self.alpha = alpha
        self.momentum = momentum

        # Choosing Adam optimizer for both generator and discriminator to feed in to the model:
        self.optimizer = Adam(adam_learning_rate, adam_beta_1)  # Values from the EEG GAN paper found to be most optimal

        # Builds the Generator, Discriminator, and the combined models:
        self.generator = self.make_generator()

        self.discriminator = self.make_discriminator()
        self.discriminator.compile(loss='binary_crossentropy', optimizer=self.optimizer, metrics=['accuracy'])

        self.combined = self.builder()
        self.combined.compile(loss='binary_crossentropy', optimizer=self.optimizer)

        # Useful for creating a sample directory later
        self.dir = 'EEG_samples'

    def forward(self, waveforms):
        return self.train(waveforms)

    def make_generator(self):
        '''
        Creates a generator model that takes in randomly generated noise, then uses 
        3 Dense layers to return a generated EEG signal that is fed into the discriminator,
        which then distinguishes whether or not it is real or fake.
        :return: Model date tuple(generated noise, eeg img)
        '''
        model = Sequential()

        model.add(Dense(256, input_dim=self.noise))
        model.add(LeakyReLU(alpha=self.alpha))
        model.add(BatchNormalization(momentum=self.momentum))
        model.add(Dense(512))
        model.add(LeakyReLU(alpha=self.alpha))
        model.add(Dense(1024))
        model.add(LeakyReLU(alpha=self.alpha))
        model.add(BatchNormalization(momentum=self.momentum))
        model.add(Dense(np.prod(self.eeg_shape), activation='tanh'))
        model.add(Reshape(self.eeg_shape))
        assert model.output_shape == (None, 500, self.channels)  # Updated to match the new shape

        noise = Input(shape=(self.noise,))
        img = model(noise)

        return Model(noise, img)

    def make_discriminator(self):
        '''
        Creates a discriminator model that distinguishes between real and generated EEG signals.
        :return: Model data tuple (image from generator, validity [true or false])
        '''
        model = Sequential()

        model.add(Flatten(input_shape=self.eeg_shape))
        model.add(Dense(512))
        model.add(LeakyReLU(alpha=self.alpha))
        model.add(Dense(256))
        model.add(LeakyReLU(alpha=self.alpha))
        model.add(Dense(1, activation='sigmoid'))

        img = Input(shape=self.eeg_shape)
        validity = model(img)

        return Model(img, validity)

    def builder(self):
        '''
        Builds the combined generator + discriminator model, where the generator tries to fool
        the discriminator into believing the generated EEG signals are real.
        :return: Model data tuple(noise signal 'z', validity[True/False])
        '''

        z = Input(shape=(self.noise,))
        generated_eeg = self.generator(z)

        discriminator = self.discriminator
        discriminator.trainable = False

        validity = discriminator(generated_eeg)

        return Model(z, validity)

    def train(self, dataset, loss_threshold=10000, start_appending_epoch=1):
        '''
        Training function for the GAN model. It alternates between training the discriminator
        and the generator over several epochs.
        :param dataset: input/training dataset
        :return: Generated EEG data
        '''

        valid = np.ones((self.batchsize, 1))
        fake = np.zeros((self.batchsize, 1))

        gen_loss = []
        disc_loss = []

        for epoch in range(self.epochs):

            # Discriminator Training Loop
            idx = np.random.randint(0, dataset.shape[0], self.batchsize)
            signal = dataset[idx]

            noise = np.random.normal(0, 1, (self.batchsize, self.noise))

            gen_signals = self.generator.predict(noise)

            d_loss_real = self.discriminator.train_on_batch(signal, valid)
            d_loss_fake = self.discriminator.train_on_batch(gen_signals, fake)
            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)

            # Generator Training Loop
            noise = np.random.normal(0, 1, (self.batchsize, self.noise))

            # Combined model training where generator is trained to fool the discriminator
            g_loss = self.combined.train_on_batch(noise, valid)

            gen_loss.append(g_loss)
            disc_loss.append(d_loss[0])

        noise = tf.random.normal([dataset.shape[0], self.noise])
        generated_data = self.generator(noise, training=False)
        return generated_data

# #Example usage:
# gan = GAN_EEG_v4()

# #Use the provided signal
# signal = torch.ones((8, 500, 17)) * torch.arange(8).reshape((8, 1, 1,))

# augmented_data = gan.forward(signal)

# print(signal.shape)
# print(augmented_data.shape)

